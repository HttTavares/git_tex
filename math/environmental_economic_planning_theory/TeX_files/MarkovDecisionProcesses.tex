\chapter{Markov Decision Processes}


\section{Markov Processes}

\section{Rewards and Policies}

\section{Markov Decision Processes}

A Markov decision Process is a tuple $\mathcal{D} = (S, P, A, R)$ where

\begin{enumerate}
	\item $S$ is a set called state space, whose elements are called states,
	\item $P \in \text{End}(A, S)$ is a stochastic matrix (define this) called probability transition matrix,
	\item $A$ is a set, called action set (or space), whose elements are called actions,
	\item $R: S \times A \times S \raw \bb{R}$ is a function, called reward function.
	
\end{enumerate}

A technical point is that it this definition is equivalent to assuming $A = \dot\cup A(i)$, for $i \in S$, where $A(i)$ denotes the set of possible actions in state $i$. In fact, we do interchange notation in the following because either version has its advantages in given situations. For instance, we can denote $(i, a) \in S \times A$ to 



\begin{exmp}
	Let $S = \{1, 2, 3 \}$, $P$
\end{exmp}
(finish)

\begin{problem}
	The Resource Allocation Markov Decision Process.
	
	Let $a $.
\end{problem}

% Sustainable Resource Allocation MDP
% 
%
%
%
%
%
%
%
%
%












